{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !git clone https://github.com/facebookresearch/av_hubert.git\n",
    "\n",
    "# %cd av_hubert\n",
    "# !git submodule init\n",
    "# !git submodule update\n",
    "# !pip install scipy\n",
    "# !pip install sentencepiece\n",
    "# !pip install python_speech_features\n",
    "# !pip install scikit-video\n",
    "\n",
    "# %cd fairseq\n",
    "# !pip install ./"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import cv2\n",
    "# net = cv2.dnn.readNetFromONNX('../../avhencoder.onnx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# sample = {'audio': np.random.randn(1, 104, 500),\n",
    "#                           'video': np.random.randn(1, 1, 500, 112, 112)}\n",
    "# net.forward([np.random.randn(1,104,500), np.random.randn(1,1,500,112,112)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2022-07-17 10:57:17--  https://dl.fbaipublicfiles.com/avhubert/model/lrs3_vox/vsr/base_vox_433h.pt\n",
      "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 104.22.75.142, 104.22.74.142, 172.67.9.4, ...\n",
      "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|104.22.75.142|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1928063847 (1.8G) [binary/octet-stream]\n",
      "Saving to: ‘finetune-model.pt’\n",
      "\n",
      "finetune-model.pt     0%[                    ]  13.64M  4.60MB/s               ^C\n"
     ]
    }
   ],
   "source": [
    "!wget https://dl.fbaipublicfiles.com/avhubert/model/lrs3_vox/vsr/base_vox_433h.pt -O finetune-model.pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 2] No such file or directory: 'av_hubert/avhubert'\n",
      "/mnt/c/Codes/GSoC/GSoC22/Github_updates/av_hubert/avhubert\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "Model file not found: ../finetune-model.pt",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-7a56688407da>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0mgen_subset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"test\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0mgen_cfg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGenerationConfig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeam\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0mmodels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaved_cfg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheckpoint_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model_ensemble_and_task\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mckpt_path\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0mmodels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;31m# saved_cfg.task.modalities = modalities\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/fairseq/checkpoint_utils.py\u001b[0m in \u001b[0;36mload_model_ensemble_and_task\u001b[0;34m(filenames, arg_overrides, task, strict, suffix, num_shards, state)\u001b[0m\n\u001b[1;32m    406\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    407\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mPathManager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 408\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Model file not found: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    409\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mstate\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    410\u001b[0m                 \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_checkpoint_to_cpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg_overrides\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: Model file not found: ../finetune-model.pt"
     ]
    }
   ],
   "source": [
    "%cd av_hubert/avhubert\n",
    "import cv2\n",
    "import tempfile\n",
    "from argparse import Namespace\n",
    "import fairseq\n",
    "from fairseq import checkpoint_utils, options, tasks, utils\n",
    "from fairseq.dataclass.configs import GenerationConfig\n",
    "from IPython.display import HTML\n",
    "import torch.onnx\n",
    "import time\n",
    "\n",
    "video_path, ckpt_path = \"../roi.mp4\", \"../finetune-model.pt\"\n",
    "user_dir = \"./\"\n",
    "num_frames = int(cv2.VideoCapture(video_path).get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "data_dir = tempfile.mkdtemp()\n",
    "tsv_cont = [\"/\\n\", f\"test-0\\t{video_path}\\t{None}\\t{num_frames}\\t{int(16_000*num_frames/25)}\\n\"]\n",
    "label_cont = [\"DUMMY\\n\"]\n",
    "with open(f\"{data_dir}/test.tsv\", \"w\") as fo:\n",
    "    fo.write(\"\".join(tsv_cont))\n",
    "with open(f\"{data_dir}/test.wrd\", \"w\") as fo:\n",
    "    fo.write(\"\".join(label_cont))\n",
    "utils.import_user_module(Namespace(user_dir=user_dir))\n",
    "modalities = [\"video\", \"audio\"]\n",
    "gen_subset = \"test\"\n",
    "gen_cfg = GenerationConfig(beam=20)\n",
    "models, saved_cfg, task = checkpoint_utils.load_model_ensemble_and_task([ckpt_path])\n",
    "models = [model.eval() for model in models]\n",
    "# saved_cfg.task.modalities = modalities\n",
    "# saved_cfg.task.data = data_dir\n",
    "# saved_cfg.task.label_dir = data_dir\n",
    "# task = tasks.setup_task(saved_cfg.task)\n",
    "# task.load_dataset(gen_subset, task_cfg=saved_cfg.task)\n",
    "# generator = task.build_generator(models, gen_cfg)\n",
    "\n",
    "# # print(generator)\n",
    "\n",
    "# def decode_fn(x):\n",
    "#     dictionary = task.target_dictionary\n",
    "#     symbols_ignore = generator.symbols_to_strip_from_output\n",
    "#     symbols_ignore.add(dictionary.pad())\n",
    "#     return task.datasets[gen_subset].label_processors[0].decode(x, symbols_ignore)\n",
    "\n",
    "# itr = task.get_batch_iterator(dataset=task.dataset(gen_subset)).next_epoch_itr(shuffle=False)\n",
    "# sample = next(itr)\n",
    "# sample = utils.move_to_cuda(sample)\n",
    "# hypos = task.inference_step(generator, models, sample)\n",
    "# ref = decode_fn(sample['target'][0].int().cpu())\n",
    "# hypo = hypos[0][0]['tokens'].int().cpu()\n",
    "# hypo = decode_fn(hypo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/c/Codes/GSoC/GSoC22/Github_updates/av_hubert/avhubert/hubert.py:602: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if mask and self.masking_type == 'input':\n",
      "/home/shiv/.local/lib/python3.8/site-packages/fairseq/modules/multihead_attention.py:151: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  assert embed_dim == self.embed_dim\n",
      "/home/shiv/.local/lib/python3.8/site-packages/fairseq/modules/multihead_attention.py:152: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  assert list(query.size()) == [tgt_len, bsz, embed_dim]\n",
      "/home/shiv/.local/lib/python3.8/site-packages/fairseq/modules/multihead_attention.py:156: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  assert key_bsz == bsz\n",
      "/home/shiv/.local/lib/python3.8/site-packages/fairseq/modules/multihead_attention.py:158: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  assert src_len, bsz == value.shape[:2]\n",
      "/home/shiv/.local/lib/python3.8/site-packages/fairseq/modules/multihead_attention.py:301: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  assert k.size(1) == src_len\n",
      "/home/shiv/.local/lib/python3.8/site-packages/fairseq/modules/multihead_attention.py:335: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  assert list(attn_weights.size()) == [bsz * self.num_heads, tgt_len, src_len]\n",
      "/home/shiv/.local/lib/python3.8/site-packages/fairseq/modules/multihead_attention.py:368: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  assert list(attn.size()) == [bsz * self.num_heads, tgt_len, self.head_dim]\n",
      "/home/shiv/.local/lib/python3.8/site-packages/fairseq/modules/multihead_attention.py:369: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if self.onnx_trace and attn.size(1) == 1:\n",
      "/mnt/c/Codes/GSoC/GSoC22/Github_updates/av_hubert/avhubert/hubert.py:653: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if features_only:\n",
      "/home/shiv/.local/lib/python3.8/site-packages/torch/onnx/symbolic_helper.py:719: UserWarning: allowzero=0 by default. In order to honor zero value in shape use allowzero=1\n",
      "  warnings.warn(\"allowzero=0 by default. In order to honor zero value in shape use allowzero=1\")\n"
     ]
    }
   ],
   "source": [
    "# models[0].encoder\n",
    "source ={'audio': torch.randn(1, 104, 500),\n",
    "                          'video': torch.randn(1, 1, 500, 112, 112)}\n",
    "models[0].encoder.w2v_model.prepare_for_onnx_export_()\n",
    "# Had to do a lot of hardcoding to make it work\n",
    "torch.onnx.export(models[0].encoder.w2v_model.cpu(), (source, {\"mask\":False, 'features_only':True}), '../encoder.onnx', opset_version=15, input_names=['audio', 'video'], output_names=['output'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try to load model and run it directly without importing anything. If ther's some ssue then changing the sequencegenrator will work otherwise not. We'll have to convert it using some other mechnanism."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
